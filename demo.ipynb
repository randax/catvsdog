{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an image classification model using very little data  \n",
    "\n",
    "This tutorial presents several ways to build an image classifier using Keras from just a few hundred or thousand pictures from each class you want to be able to recognize.\n",
    "\n",
    "We will go over the following options:  \n",
    "\n",
    "- training a small network from scratch (as a baseline)  \n",
    "- using the bottleneck features of a pre-trained network  \n",
    "- fine-tuning the top layers of a pre-trained network  \n",
    "  \n",
    "This will lead us to cover the following Keras features:   \n",
    "  \n",
    "- fit_generator for training Keras a model using Python data generators  \n",
    "- ImageDataGenerator for real-time data augmentation  \n",
    "- layer freezing and model fine-tuning  \n",
    "- ...and more.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step.1 - Install libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install --name root tensorflow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.version import StrictVersion\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# should at least be 1.0\n",
    "assert StrictVersion(tf.__version__) >= StrictVersion('1.0.0')\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as keras\n",
    "\n",
    "# should at least be 1.2 and use TensorFlow backend\n",
    "assert StrictVersion(keras.__version__) >= StrictVersion('1.2.2')\n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -U numpy\n",
    "!python -m pip install -U scipy\n",
    "!python -m pip install -U Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is a subset of the dogs vs cats dataset\n",
    "https://www.kaggle.com/c/dogs-vs-cats/data  \n",
    "Made a folder stucture and added the relevant data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "data/\n",
    "    train/\n",
    "        dogs/ ### 1024 pictures\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/ ### 1024 pictures\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/ ### 416 pictures\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/ ### 416 pictures\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The github repo includes about 3000 images, but the holde dataset is about 12500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2048 images belonging to 2 classes.\n",
      "Found 832 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# used to rescale the pixel values from [0, 255] to [0, 1] interval\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# automagically retrieve images and their classes for train and validation sets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=16,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What inspired Convolutional Networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs are biologically-inspired models inspired by research by D. H. Hubel and T. N. Wiesel. They proposed an explanation for the way in which mammals visually perceive the world around them using a layered architecture of neurons in the brain, and this in turn inspired engineers to attempt to develop similar pattern recognition mechanisms in computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://qph.ec.quoracdn.net/main-qimg-235acb60a481423eaf70c39b17bc914b.webp \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their hypothesis, within the visual cortex, complex functional responses generated by \"complex cells\" are constructed from more simplistic responses from \"simple cells'. \n",
    "\n",
    "For instances, simple cells would respond to oriented edges etc, while complex cells will also respond to oriented edges but with a degree of spatial invariance.\n",
    "\n",
    "Receptive fields exist for cells, where a cell responds to a summation of inputs from other local cells.\n",
    "\n",
    "The architecture of deep convolutional neural networks was inspired by the ideas mentioned above \n",
    "- local connections \n",
    "- layering  \n",
    "- spatial invariance (shifting the input signal results in an equally shifted output signal. , most of us are able to recognize specific faces under a variety of conditions because we learn abstraction These abstractions are thus invariant to size, contrast, rotation, orientation\n",
    " \n",
    "However, it remains to be seen if these computational mechanisms of convolutional neural networks are similar to the computation mechanisms occurring in the primate visual system\n",
    "\n",
    "- convolution operation\n",
    "- shared weights\n",
    "- pooling/subsampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it work? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://images.nature.com/w926/nature-assets/srep/2016/160610/srep27755/images_hires/srep27755-f1.jpg \"Logo Title Text 1\")\n",
    "![alt text](https://www.mathworks.com/content/mathworks/www/en/discovery/convolutional-neural-network/jcr:content/mainParsys/image_copy.adapt.full.high.jpg/1497876372993.jpg \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Convolution \n",
    "\n",
    "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/more_images/Convolution_schematic.gif \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure_2.png \"Logo Title Text 1\")\n",
    "\n",
    "- A convolution is an orderly procedure where two sources of information are intertwined.\n",
    "\n",
    "- A kernel (also called a filter) is a smaller-sized matrix in comparison to the input dimensions of the image, that consists of real valued entries.\n",
    "\n",
    "- Kernels are then convolved with the input volume to obtain so-called ‘activation maps’ (also called feature maps).  \n",
    "- Activation maps indicate ‘activated’ regions, i.e. regions where features specific to the kernel have been detected in the input. \n",
    "\n",
    "- The real values of the kernel matrix change with each learning iteration over the training set, indicating that the network is learning to identify which regions are of significance for extracting features from the data.\n",
    "\n",
    "- We compute the dot product between the kernel and the input matrix. -The convolved value obtained by summing the resultant terms from the dot product forms a single entry in the activation matrix. \n",
    "\n",
    "- The patch selection is then slided (towards the right, or downwards when the boundary of the matrix is reached) by a certain amount called the ‘stride’ value, and the process is repeated till the entire input image has been processed. - The process is carried out for all colour channels.\n",
    "\n",
    "- instead of connecting each neuron to all possible pixels, we specify a 2 dimensional region called the ‘receptive field[14]’ (say of size 5×5 units) extending to the entire depth of the input (5x5x3 for a 3 colour channel input), within which the encompassed pixels are fully connected to the neural network’s input layer. It’s over these small regions that the network layer cross-sections (each consisting of several neurons (called ‘depth columns’)) operate and produce the activation map. (reduces computational complexity)\n",
    "\n",
    "![alt text](http://i.imgur.com/g4hRI6Z.png \"Logo Title Text 1\")\n",
    "![alt text](http://i.imgur.com/tpQvMps.jpg \"Logo Title Text 1\")\n",
    "![alt text](http://i.imgur.com/oyXkhHi.jpg \"Logo Title Text 1\")\n",
    "![alt text](http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure_5.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 3 - Pooling\n",
    "![alt text](http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure_6.png \"Logo Title Text 1\")\n",
    "\n",
    "- Pooling reducing the spatial dimensions (Width x Height) of the Input Volume for the next Convolutional Layer. It does not affect the depth dimension of the Volume.  \n",
    "- The transformation is either performed by taking the maximum value from the values observable in the window (called ‘max pooling’), or by taking the average of the values. Max pooling has been favoured over others due to its better performance characteristics.\n",
    "- also called downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 4 - Normalization (ReLU in our case)\n",
    "\n",
    "![alt text](http://xrds.acm.org/blog/wp-content/uploads/2016/06/CodeCogsEqn-3.png \"Logo Title Text 1\")\n",
    "\n",
    "Normalization (keep the math from breaking by turning all negative numbers to 0)  (RELU) a stack of images becomes a stack of images with no negative values. \n",
    "\n",
    "Repeat Steps 2-4 several times. More, smaller images (feature maps created at every layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Regularization \n",
    "\n",
    "- Dropout forces an artificial neural network to learn multiple independent representations of the same data by alternately randomly disabling neurons in the learning phase.\n",
    "- Dropout is a vital feature in almost every state-of-the-art neural network implementation.\n",
    "- To perform dropout on a layer, you randomly set some of the layer's values to 0 during forward propagation.\n",
    "\n",
    "See [this](http://iamtrask.github.io/2015/07/28/dropout/)\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/CewjH.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 6 - Probability Conversion\n",
    "\n",
    "At the very end of our network (the tail), we'll apply a softmax function to convert the outputs to probability values for each class. \n",
    "\n",
    "![alt text](https://1.bp.blogspot.com/-FHDU505euic/Vs1iJjXHG0I/AAAAAAABVKg/x4g0FHuz7_A/s1600/softmax.JPG \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So how do we learn the magic numbers? \n",
    "\n",
    "- We can learn features and weight values through backpropagation\n",
    "\n",
    "![alt text](http://www.robots.ox.ac.uk/~vgg/practicals/cnn/images/cover.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/cnn-toupload-final-151117124948-lva1-app6892/95/convolutional-neural-networks-cnn-52-638.jpg?cb=1455889178 \"Logo Title Text 1\")\n",
    "\n",
    "The other hyperparameters are set by humans and they are an active field of research (finding the optimal ones)\n",
    "\n",
    "i.e -  number of neurons, number of features, size of features, poooling window size, window stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Conv Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(150, 150,...)`\n",
      "  \n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
      "  \n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(img_width, img_height,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are here using a Sequential model since to make the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 30\n",
    "nb_train_samples = 2048\n",
    "nb_validation_samples = 832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=<keras.pre..., steps_per_epoch=128, epochs=30, validation_steps=832)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "128/128 [==============================] - 248s - loss: 0.7023 - acc: 0.5376 - val_loss: 0.6534 - val_acc: 0.6370\n",
      "Epoch 2/30\n",
      "128/128 [==============================] - 251s - loss: 0.6410 - acc: 0.6523 - val_loss: 0.6467 - val_acc: 0.6346\n",
      "Epoch 3/30\n",
      "128/128 [==============================] - 252s - loss: 0.5958 - acc: 0.6914 - val_loss: 0.5991 - val_acc: 0.6970\n",
      "Epoch 4/30\n",
      "128/128 [==============================] - 249s - loss: 0.5379 - acc: 0.7271 - val_loss: 0.6053 - val_acc: 0.6909\n",
      "Epoch 5/30\n",
      "128/128 [==============================] - 250s - loss: 0.4901 - acc: 0.7676 - val_loss: 0.5597 - val_acc: 0.7381\n",
      "Epoch 6/30\n",
      "128/128 [==============================] - 249s - loss: 0.4339 - acc: 0.7959 - val_loss: 0.6061 - val_acc: 0.7203\n",
      "Epoch 7/30\n",
      "128/128 [==============================] - 259s - loss: 0.3799 - acc: 0.8306 - val_loss: 0.5857 - val_acc: 0.7205\n",
      "Epoch 8/30\n",
      "128/128 [==============================] - 256s - loss: 0.3265 - acc: 0.8623 - val_loss: 0.6181 - val_acc: 0.7358\n",
      "Epoch 9/30\n",
      "128/128 [==============================] - 243s - loss: 0.2699 - acc: 0.8833 - val_loss: 0.6647 - val_acc: 0.7246\n",
      "Epoch 10/30\n",
      "128/128 [==============================] - 251s - loss: 0.2191 - acc: 0.9102 - val_loss: 0.9544 - val_acc: 0.6972\n",
      "Epoch 11/30\n",
      "128/128 [==============================] - 247s - loss: 0.2007 - acc: 0.9136 - val_loss: 0.9272 - val_acc: 0.7280\n",
      "Epoch 12/30\n",
      "128/128 [==============================] - 260s - loss: 0.1815 - acc: 0.9292 - val_loss: 0.9721 - val_acc: 0.7331\n",
      "Epoch 13/30\n",
      "128/128 [==============================] - 256s - loss: 0.1491 - acc: 0.9424 - val_loss: 1.1978 - val_acc: 0.7211\n",
      "Epoch 14/30\n",
      "128/128 [==============================] - 262s - loss: 0.1252 - acc: 0.9521 - val_loss: 1.3130 - val_acc: 0.7347\n",
      "Epoch 15/30\n",
      "128/128 [==============================] - 252s - loss: 0.1120 - acc: 0.9595 - val_loss: 1.1796 - val_acc: 0.7086\n",
      "Epoch 16/30\n",
      "128/128 [==============================] - 252s - loss: 0.0944 - acc: 0.9658 - val_loss: 1.2708 - val_acc: 0.7307\n",
      "Epoch 17/30\n",
      "128/128 [==============================] - 253s - loss: 0.0881 - acc: 0.9683 - val_loss: 1.6406 - val_acc: 0.7042\n",
      "Epoch 18/30\n",
      "128/128 [==============================] - 264s - loss: 0.0910 - acc: 0.9727 - val_loss: 1.4259 - val_acc: 0.7075\n",
      "Epoch 19/30\n",
      "128/128 [==============================] - 249s - loss: 0.0701 - acc: 0.9736 - val_loss: 1.9290 - val_acc: 0.7020\n",
      "Epoch 20/30\n",
      "128/128 [==============================] - 254s - loss: 0.0923 - acc: 0.9707 - val_loss: 3.0574 - val_acc: 0.6430\n",
      "Epoch 21/30\n",
      "128/128 [==============================] - 249s - loss: 0.0801 - acc: 0.9751 - val_loss: 1.7892 - val_acc: 0.7058\n",
      "Epoch 22/30\n",
      "128/128 [==============================] - 248s - loss: 0.0830 - acc: 0.9731 - val_loss: 1.6470 - val_acc: 0.7011\n",
      "Epoch 23/30\n",
      "128/128 [==============================] - 254s - loss: 0.1046 - acc: 0.9683 - val_loss: 2.3000 - val_acc: 0.6885\n",
      "Epoch 24/30\n",
      "128/128 [==============================] - 252s - loss: 0.1060 - acc: 0.9707 - val_loss: 1.4874 - val_acc: 0.7064\n",
      "Epoch 25/30\n",
      "128/128 [==============================] - 248s - loss: 0.0937 - acc: 0.9697 - val_loss: 1.7718 - val_acc: 0.7092\n",
      "Epoch 26/30\n",
      "128/128 [==============================] - 261s - loss: 0.0971 - acc: 0.9722 - val_loss: 2.2591 - val_acc: 0.7198\n",
      "Epoch 27/30\n",
      "128/128 [==============================] - 256s - loss: 0.0980 - acc: 0.9712 - val_loss: 1.5536 - val_acc: 0.7260\n",
      "Epoch 28/30\n",
      "128/128 [==============================] - 254s - loss: 0.1101 - acc: 0.9717 - val_loss: 1.8053 - val_acc: 0.7174\n",
      "Epoch 29/30\n",
      "128/128 [==============================] - 247s - loss: 0.1118 - acc: 0.9673 - val_loss: 1.6431 - val_acc: 0.7058\n",
      "Epoch 30/30\n",
      "128/128 [==============================] - 258s - loss: 0.1211 - acc: 0.9644 - val_loss: 2.0211 - val_acc: 0.7089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x117febcf8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('models/basic_cnn_20_epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('models_trained/basic_cnn_20_epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model successfully runs at one epoch, go back and it for 30 epochs by changing nb_epoch above.  I was able to get to an val_acc of 0.71 at 30 epochs.\n",
    "A copy of a pretrained network is available in the pretrained folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing loss and accuracy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0198164684518884, 0.70905949519230771]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(validation_generator, nb_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolution of accuracy on training (blue) and validation (green) sets for 1 to 32 epochs :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Accuracy evolution](pictures/scores_no_dataaugmentation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After ~10 epochs the neural network reach ~70% accuracy. We can witness overfitting, no progress is made over validation set in the next epochs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation for improving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying random transformation to our train set, we artificially enhance our dataset with new unseen images.  \n",
    "This will hopefully reduce overfitting and allows better generalization capability for our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of data augmentation applied to a picture:\n",
    "![Example of data augmentation applied to a picture](pictures/cat_data_augmentation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2048 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen_augmented = ImageDataGenerator(\n",
    "        rescale=1./255,        # normalize pixel values to [0,1]\n",
    "        shear_range=0.2,       # randomly applies shearing transformation\n",
    "        zoom_range=0.2,        # randomly applies shearing transformation\n",
    "        horizontal_flip=True)  # randomly flip the images\n",
    "\n",
    "# same code as before\n",
    "train_generator_augmented = train_datagen_augmented.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=<keras.pre..., steps_per_epoch=64, epochs=30, validation_steps=832)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "64/64 [==============================] - 250s - loss: 0.6496 - acc: 0.6899 - val_loss: 0.5904 - val_acc: 0.6994\n",
      "Epoch 2/30\n",
      "64/64 [==============================] - 253s - loss: 0.6266 - acc: 0.6870 - val_loss: 0.5643 - val_acc: 0.7514\n",
      "Epoch 3/30\n",
      "64/64 [==============================] - 249s - loss: 0.5731 - acc: 0.7412 - val_loss: 0.7995 - val_acc: 0.5404\n",
      "Epoch 4/30\n",
      "64/64 [==============================] - 258s - loss: 0.5494 - acc: 0.7285 - val_loss: 0.5845 - val_acc: 0.7220\n",
      "Epoch 5/30\n",
      "64/64 [==============================] - 278s - loss: 0.5466 - acc: 0.7432 - val_loss: 0.6365 - val_acc: 0.7562\n",
      "Epoch 6/30\n",
      "64/64 [==============================] - 289s - loss: 0.5717 - acc: 0.7339 - val_loss: 0.5228 - val_acc: 0.7428\n",
      "Epoch 7/30\n",
      "64/64 [==============================] - 273s - loss: 0.5261 - acc: 0.7588 - val_loss: 0.6112 - val_acc: 0.7569\n",
      "Epoch 8/30\n",
      "64/64 [==============================] - 266s - loss: 0.5234 - acc: 0.7695 - val_loss: 0.5437 - val_acc: 0.7554\n",
      "Epoch 9/30\n",
      "64/64 [==============================] - 254s - loss: 0.5066 - acc: 0.7764 - val_loss: 0.5465 - val_acc: 0.7533\n",
      "Epoch 10/30\n",
      "64/64 [==============================] - 263s - loss: 0.4935 - acc: 0.7837 - val_loss: 0.5153 - val_acc: 0.7609\n",
      "Epoch 11/30\n",
      "64/64 [==============================] - 271s - loss: 0.4948 - acc: 0.7656 - val_loss: 0.5359 - val_acc: 0.7536\n",
      "Epoch 12/30\n",
      "64/64 [==============================] - 333s - loss: 0.5037 - acc: 0.7671 - val_loss: 0.8214 - val_acc: 0.7777\n",
      "Epoch 13/30\n",
      "64/64 [==============================] - 444s - loss: 0.4933 - acc: 0.7808 - val_loss: 0.5173 - val_acc: 0.7396\n",
      "Epoch 14/30\n",
      "64/64 [==============================] - 449s - loss: 0.4665 - acc: 0.7871 - val_loss: 0.5111 - val_acc: 0.7571\n",
      "Epoch 15/30\n",
      "64/64 [==============================] - 465s - loss: 0.4736 - acc: 0.7910 - val_loss: 0.5181 - val_acc: 0.7725\n",
      "Epoch 16/30\n",
      "64/64 [==============================] - 430s - loss: 0.4778 - acc: 0.7920 - val_loss: 0.4945 - val_acc: 0.7826\n",
      "Epoch 17/30\n",
      "64/64 [==============================] - 432s - loss: 0.4713 - acc: 0.7886 - val_loss: 0.5096 - val_acc: 0.7400\n",
      "Epoch 18/30\n",
      "64/64 [==============================] - 443s - loss: 0.4454 - acc: 0.8047 - val_loss: 0.5760 - val_acc: 0.7776\n",
      "Epoch 19/30\n",
      "64/64 [==============================] - 449s - loss: 0.4697 - acc: 0.7910 - val_loss: 0.5088 - val_acc: 0.7829\n",
      "Epoch 20/30\n",
      "64/64 [==============================] - 467s - loss: 0.4706 - acc: 0.7949 - val_loss: 0.5456 - val_acc: 0.7545\n",
      "Epoch 21/30\n",
      "64/64 [==============================] - 463s - loss: 0.4392 - acc: 0.8101 - val_loss: 0.4964 - val_acc: 0.7738\n",
      "Epoch 22/30\n",
      "64/64 [==============================] - 430s - loss: 0.4449 - acc: 0.8027 - val_loss: 0.6729 - val_acc: 0.7873\n",
      "Epoch 23/30\n",
      "64/64 [==============================] - 426s - loss: 0.4486 - acc: 0.8066 - val_loss: 0.8184 - val_acc: 0.7414\n",
      "Epoch 24/30\n",
      "64/64 [==============================] - 389s - loss: 0.4253 - acc: 0.8140 - val_loss: 0.4647 - val_acc: 0.7954\n",
      "Epoch 25/30\n",
      "64/64 [==============================] - 241s - loss: 0.4542 - acc: 0.7949 - val_loss: 0.4931 - val_acc: 0.8007\n",
      "Epoch 26/30\n",
      "64/64 [==============================] - 245s - loss: 0.4340 - acc: 0.8105 - val_loss: 0.4770 - val_acc: 0.7970\n",
      "Epoch 27/30\n",
      "64/64 [==============================] - 243s - loss: 0.4534 - acc: 0.8057 - val_loss: 0.6081 - val_acc: 0.7509\n",
      "Epoch 28/30\n",
      "64/64 [==============================] - 249s - loss: 0.4252 - acc: 0.8140 - val_loss: 0.5880 - val_acc: 0.7774\n",
      "Epoch 29/30\n",
      "64/64 [==============================] - 252s - loss: 0.4147 - acc: 0.8218 - val_loss: 0.4870 - val_acc: 0.7827\n",
      "Epoch 30/30\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.4323 - acc: 0.8180"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator_augmented,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('models/augmented_30_epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('models_trained/augmented_30_epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing loss and accuracy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.69096329235113585, 0.76802884615384615]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(validation_generator, nb_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolution of accuracy on training (blue) and validation (green) sets for 1 to 100 epochs :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Accuracy evolution](pictures/scores_with_dataaugmentation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thanks to data-augmentation, the accuracy on the validation set improved to ~80%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of training a convolutionnal neural network can be very time-consuming and require a lot of datas.  \n",
    "\n",
    "We can go beyond the previous models in terms of performance and efficiency by using a general-purpose, pre-trained image classifier.  This example uses VGG16, a model trained on the ImageNet dataset - which contains millions of images classified in 1000 categories. \n",
    "\n",
    "On top of it, we add a small multi-layer perceptron and we train it on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 + small MLP\n",
    "![VGG16 + Dense layers Schema](pictures/vgg16_original.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG16 model architecture definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg = Sequential()\n",
    "model_vgg.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height,3)))\n",
    "model_vgg.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading VGG16 weights\n",
    "This part is a bit complicated because the structure of our model is not exactly the same as the one used when training weights.  \n",
    "Otherwise, we would use the `model.load_weights()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File('models/vgg16_weights.h5')\n",
    "for k in range(f.attrs['nb_layers']):\n",
    "    if k >= len(model_vgg.layers) - 1:\n",
    "        # we don't look at the last two layers in the savefile (fully-connected and activation)\n",
    "        break\n",
    "    g = f['layer_{}'.format(k)]\n",
    "    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "    layer = model_vgg.layers[k]\n",
    "\n",
    "    if layer.__class__.__name__ in ['Convolution1D', 'Convolution2D', 'Convolution3D', 'AtrousConvolution2D']:\n",
    "        weights[0] = np.transpose(weights[0], (2, 3, 1, 0))\n",
    "\n",
    "    layer.set_weights(weights)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the VGG16 model to process samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2048 images belonging to 2 classes.\n",
      "Found 832 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator_bottleneck = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "validation_generator_bottleneck = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode=None,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a long process, so we save the output of the VGG16 once and for all.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_train = model_vgg.predict_generator(train_generator_bottleneck, nb_train_samples)\n",
    "np.save(open('models/bottleneck_features_train.npy', 'wb'), bottleneck_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_validation = model_vgg.predict_generator(validation_generator_bottleneck, nb_validation_samples)\n",
    "np.save(open('models/bottleneck_features_validation.npy', 'wb'), bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(open('models/bottleneck_features_train.npy', 'rb'))\n",
    "train_labels = np.array([0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
    "\n",
    "validation_data = np.load(open('models/bottleneck_features_validation.npy', 'rb'))\n",
    "validation_labels = np.array([0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define and train the custom fully connected neural network :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_top = Sequential()\n",
    "model_top.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "model_top.add(Dense(256, activation='relu'))\n",
    "model_top.add(Dropout(0.5))\n",
    "model_top.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_top.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2048 samples, validate on 832 samples\n",
      "Epoch 1/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.9784 - acc: 0.6855 - val_loss: 0.4140 - val_acc: 0.8077\n",
      "Epoch 2/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.4831 - acc: 0.7891 - val_loss: 0.3646 - val_acc: 0.8365\n",
      "Epoch 3/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.3974 - acc: 0.8271 - val_loss: 0.3369 - val_acc: 0.8558\n",
      "Epoch 4/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.3438 - acc: 0.8525 - val_loss: 0.3136 - val_acc: 0.8594\n",
      "Epoch 5/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.3131 - acc: 0.8647 - val_loss: 0.5777 - val_acc: 0.7873\n",
      "Epoch 6/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.2883 - acc: 0.8823 - val_loss: 0.2913 - val_acc: 0.8786\n",
      "Epoch 7/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.2321 - acc: 0.9048 - val_loss: 0.3724 - val_acc: 0.8462\n",
      "Epoch 8/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.2057 - acc: 0.9092 - val_loss: 0.4276 - val_acc: 0.8438\n",
      "Epoch 9/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.2000 - acc: 0.9141 - val_loss: 0.3741 - val_acc: 0.8618\n",
      "Epoch 10/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.1811 - acc: 0.9248 - val_loss: 0.3447 - val_acc: 0.8726\n",
      "Epoch 11/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.1540 - acc: 0.9395 - val_loss: 0.3352 - val_acc: 0.8714\n",
      "Epoch 12/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.1511 - acc: 0.9414 - val_loss: 0.3706 - val_acc: 0.8738\n",
      "Epoch 13/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.1362 - acc: 0.9399 - val_loss: 0.3913 - val_acc: 0.8798\n",
      "Epoch 14/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.1134 - acc: 0.9497 - val_loss: 0.3854 - val_acc: 0.8726\n",
      "Epoch 15/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.1117 - acc: 0.9609 - val_loss: 0.5750 - val_acc: 0.8534\n",
      "Epoch 16/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0998 - acc: 0.9565 - val_loss: 0.4339 - val_acc: 0.8870\n",
      "Epoch 17/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0678 - acc: 0.9736 - val_loss: 0.4881 - val_acc: 0.8762\n",
      "Epoch 18/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0852 - acc: 0.9658 - val_loss: 0.4335 - val_acc: 0.8870\n",
      "Epoch 19/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0733 - acc: 0.9692 - val_loss: 0.4505 - val_acc: 0.8750\n",
      "Epoch 20/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0616 - acc: 0.9756 - val_loss: 0.5825 - val_acc: 0.8606\n",
      "Epoch 21/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0585 - acc: 0.9771 - val_loss: 0.6856 - val_acc: 0.8510\n",
      "Epoch 22/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0494 - acc: 0.9800 - val_loss: 0.5624 - val_acc: 0.8750\n",
      "Epoch 23/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0464 - acc: 0.9839 - val_loss: 0.8560 - val_acc: 0.8401\n",
      "Epoch 24/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0626 - acc: 0.9727 - val_loss: 0.5666 - val_acc: 0.8738\n",
      "Epoch 25/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0285 - acc: 0.9912 - val_loss: 0.5543 - val_acc: 0.8786\n",
      "Epoch 26/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0463 - acc: 0.9805 - val_loss: 0.6137 - val_acc: 0.8738\n",
      "Epoch 27/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0260 - acc: 0.9917 - val_loss: 0.6608 - val_acc: 0.8882\n",
      "Epoch 28/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0414 - acc: 0.9824 - val_loss: 0.6423 - val_acc: 0.8702\n",
      "Epoch 29/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0308 - acc: 0.9878 - val_loss: 0.6941 - val_acc: 0.8726\n",
      "Epoch 30/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0273 - acc: 0.9897 - val_loss: 0.6677 - val_acc: 0.8678\n",
      "Epoch 31/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0315 - acc: 0.9927 - val_loss: 0.6830 - val_acc: 0.8738\n",
      "Epoch 32/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0258 - acc: 0.9922 - val_loss: 0.7593 - val_acc: 0.8798\n",
      "Epoch 33/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0266 - acc: 0.9922 - val_loss: 0.8203 - val_acc: 0.8546\n",
      "Epoch 34/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0198 - acc: 0.9941 - val_loss: 0.9712 - val_acc: 0.8450\n",
      "Epoch 35/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0291 - acc: 0.9902 - val_loss: 0.8274 - val_acc: 0.8594\n",
      "Epoch 36/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0168 - acc: 0.9941 - val_loss: 0.8353 - val_acc: 0.8738\n",
      "Epoch 37/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0324 - acc: 0.9912 - val_loss: 0.7481 - val_acc: 0.8702\n",
      "Epoch 38/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0191 - acc: 0.9932 - val_loss: 0.8011 - val_acc: 0.8750\n",
      "Epoch 39/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0254 - acc: 0.9897 - val_loss: 0.7558 - val_acc: 0.8702\n",
      "Epoch 40/40\n",
      "2048/2048 [==============================] - 0s - loss: 0.0267 - acc: 0.9897 - val_loss: 0.7828 - val_acc: 0.8798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb0b60feb90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_epoch=40\n",
    "model_top.fit(train_data, train_labels,\n",
    "          nb_epoch=nb_epoch, batch_size=32,\n",
    "          validation_data=(validation_data, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process of this small neural network is very fast : ~2s per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_top.save_weights('models/bottleneck_40_epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottleneck model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_top.load_weights('models/with-bottleneck/1000-samples--100-epochs.h5')\n",
    "#model_top.load_weights('/notebook/Data1/Code/keras-workshop/models/with-bottleneck/1000-samples--100-epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss and accuracy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704/832 [========================>.....] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.782802758165277, 0.87980769230769229]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_top.evaluate(validation_data, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolution of accuracy on training (blue) and validation (green) sets for 1 to 32 epochs :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Accuracy evolution](pictures/scores_with_bottleneck.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We reached a 90% accuracy on the validation after ~1m of training (~20 epochs) and 8% of the samples originally available on the Kaggle competition !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Fine-tuning the top layers of a a pre-trained network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Start by instantiating the VGG base and loading its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_vgg = Sequential()\n",
    "model_vgg.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height,3)))\n",
    "model_vgg.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File('models/vgg/vgg16_weights.h5')\n",
    "for k in range(f.attrs['nb_layers']):\n",
    "    if k >= len(model_vgg.layers) - 1:\n",
    "        # we don't look at the last two layers in the savefile (fully-connected and activation)\n",
    "        break\n",
    "    g = f['layer_{}'.format(k)]\n",
    "    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "    layer = model_vgg.layers[k]\n",
    "\n",
    "    if layer.__class__.__name__ in ['Convolution1D', 'Convolution2D', 'Convolution3D', 'AtrousConvolution2D']:\n",
    "        weights[0] = np.transpose(weights[0], (2, 3, 1, 0))\n",
    "\n",
    "    layer.set_weights(weights)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a classifier model to put on top of the convolutional model. For the fine tuning, we start with a fully trained-classifer. We will use the weights from the earlier model. And then we will add this model on top of the convolutional base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=model_vgg.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "top_model.load_weights('models/bottleneck_40_epochs.h5')\n",
    "\n",
    "model_vgg.add(top_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine turning, we only want to train a few layers.  This line will set the first 25 layers (up to the conv block) to non-trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_vgg.layers[:25]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model_vgg.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2048 images belonging to 2 classes.\n",
      "Found 832 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# prepare data augmentation configuration  . . . do we need this?\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "2048/2048 [==============================] - 39s - loss: 0.3593 - acc: 0.8862 - val_loss: 0.3669 - val_acc: 0.8738\n",
      "Epoch 2/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.2559 - acc: 0.9077 - val_loss: 0.3483 - val_acc: 0.8594\n",
      "Epoch 3/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.1942 - acc: 0.9292 - val_loss: 0.3380 - val_acc: 0.8618\n",
      "Epoch 4/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.1900 - acc: 0.9258 - val_loss: 0.3268 - val_acc: 0.8858\n",
      "Epoch 5/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.1638 - acc: 0.9395 - val_loss: 0.3094 - val_acc: 0.8894\n",
      "Epoch 6/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.1307 - acc: 0.9502 - val_loss: 0.3038 - val_acc: 0.8930\n",
      "Epoch 7/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.1223 - acc: 0.9556 - val_loss: 0.3267 - val_acc: 0.8990\n",
      "Epoch 8/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.1233 - acc: 0.9570 - val_loss: 0.2864 - val_acc: 0.8930\n",
      "Epoch 9/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.1147 - acc: 0.9648 - val_loss: 0.3657 - val_acc: 0.8990\n",
      "Epoch 10/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.1070 - acc: 0.9590 - val_loss: 0.3192 - val_acc: 0.8990\n",
      "Epoch 11/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0825 - acc: 0.9683 - val_loss: 0.4118 - val_acc: 0.8990\n",
      "Epoch 12/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0953 - acc: 0.9663 - val_loss: 0.3561 - val_acc: 0.9002\n",
      "Epoch 13/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0605 - acc: 0.9756 - val_loss: 0.3643 - val_acc: 0.9026\n",
      "Epoch 14/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0826 - acc: 0.9736 - val_loss: 0.3549 - val_acc: 0.8990\n",
      "Epoch 15/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0758 - acc: 0.9707 - val_loss: 0.3911 - val_acc: 0.9014\n",
      "Epoch 16/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0547 - acc: 0.9756 - val_loss: 0.4181 - val_acc: 0.9002\n",
      "Epoch 17/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0573 - acc: 0.9795 - val_loss: 0.3912 - val_acc: 0.9038\n",
      "Epoch 18/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0499 - acc: 0.9810 - val_loss: 0.4097 - val_acc: 0.9075\n",
      "Epoch 19/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0504 - acc: 0.9790 - val_loss: 0.4063 - val_acc: 0.9159\n",
      "Epoch 20/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0398 - acc: 0.9858 - val_loss: 0.4282 - val_acc: 0.9099\n",
      "Epoch 21/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0477 - acc: 0.9824 - val_loss: 0.4364 - val_acc: 0.8930\n",
      "Epoch 22/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0401 - acc: 0.9873 - val_loss: 0.4338 - val_acc: 0.9038\n",
      "Epoch 23/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0357 - acc: 0.9897 - val_loss: 0.4830 - val_acc: 0.9026\n",
      "Epoch 24/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0397 - acc: 0.9878 - val_loss: 0.4229 - val_acc: 0.9111\n",
      "Epoch 25/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0353 - acc: 0.9863 - val_loss: 0.4715 - val_acc: 0.9171\n",
      "Epoch 26/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0308 - acc: 0.9888 - val_loss: 0.4751 - val_acc: 0.9159\n",
      "Epoch 27/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0432 - acc: 0.9834 - val_loss: 0.4557 - val_acc: 0.9099\n",
      "Epoch 28/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0385 - acc: 0.9868 - val_loss: 0.4859 - val_acc: 0.9038\n",
      "Epoch 29/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0270 - acc: 0.9893 - val_loss: 0.4334 - val_acc: 0.9062\n",
      "Epoch 30/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0356 - acc: 0.9897 - val_loss: 0.4468 - val_acc: 0.9038\n",
      "Epoch 31/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0344 - acc: 0.9888 - val_loss: 0.4405 - val_acc: 0.9087\n",
      "Epoch 32/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0273 - acc: 0.9897 - val_loss: 0.5071 - val_acc: 0.9050\n",
      "Epoch 33/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0179 - acc: 0.9937 - val_loss: 0.5163 - val_acc: 0.9111\n",
      "Epoch 34/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0397 - acc: 0.9868 - val_loss: 0.4168 - val_acc: 0.9123\n",
      "Epoch 35/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0240 - acc: 0.9941 - val_loss: 0.4653 - val_acc: 0.9111\n",
      "Epoch 36/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0206 - acc: 0.9932 - val_loss: 0.5174 - val_acc: 0.9062\n",
      "Epoch 37/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0308 - acc: 0.9902 - val_loss: 0.5016 - val_acc: 0.9087\n",
      "Epoch 38/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0159 - acc: 0.9966 - val_loss: 0.4876 - val_acc: 0.9111\n",
      "Epoch 39/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0335 - acc: 0.9883 - val_loss: 0.4280 - val_acc: 0.9050\n",
      "Epoch 40/40\n",
      "2048/2048 [==============================] - 35s - loss: 0.0198 - acc: 0.9922 - val_loss: 0.4693 - val_acc: 0.9038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb059495c50>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tune the model\n",
    "model_vgg.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_vgg.save_weights('models/finetuning_20epochs_vgg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_vgg.load_weights('models/finetuning_20epochs_vgg.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing loss and accuracy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.46926221093879295, 0.90384615384615385]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vgg.evaluate_generator(validation_generator, nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.69096328031558252, 0.76802884615384615]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(validation_generator, nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704/832 [========================>.....] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.782802758165277, 0.87980769230769229]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_top.evaluate(validation_data, validation_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
